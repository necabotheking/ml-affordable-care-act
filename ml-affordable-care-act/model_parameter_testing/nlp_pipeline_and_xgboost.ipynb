{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this just in case\n",
    "nltk.download(\"all\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some models are tolerant of class imbalance\n",
    "# also, the other dataset is now imbalanced due to category selection\n",
    "# might as well use the naturally imbalanced one\n",
    "yelp_data = pd.read_csv(\"yelp_true_sample_100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # tokenizing test, ensuring it is not case insensitive\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # joining filtered tokens back into a string\n",
    "    filtered_text = \" \".join(filtered_tokens)\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "# apply function to 'text' column in yelp_data\n",
    "yelp_data[\"text\"] = yelp_data[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create binary category based on star rating\n",
    "yelp_data[\"sentiment\"] = yelp_data[\"stars\"].apply(lambda x: 0 if x <= 2 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.912"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive class rate\n",
    "(yelp_data[\"sentiment\"].sum() / 100000) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.087999999999994"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative class rate\n",
    "100 - (yelp_data[\"sentiment\"].sum() / 100000) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train/test with 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    yelp_data[\"text\"], yelp_data[\"sentiment\"], test_size=0.2, random_state=123\n",
    ")\n",
    "\n",
    "# Define TF-IDF vectorizer and fit to the training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data using the same vectorizer to prevent data leakage\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize classifier\n",
    "xgb_clf = XGBClassifier()\n",
    "\n",
    "# Define hyperparameters to tune using grid search\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.5],\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "}\n",
    "\n",
    "# Define grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit grid search to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use best hyperparameters to create the best XGBoost model\n",
    "best_xgb_clf = XGBClassifier(\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    ")\n",
    "\n",
    "# Fit the best XGBoost model to the training data\n",
    "best_xgb_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict sentiment of the testing data with model\n",
    "y_pred = best_xgb_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.5, 'max_depth': 7, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# see which hyperparameters were selected as best\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91395\n"
     ]
    }
   ],
   "source": [
    "# Calculate the raw accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.77      0.81      4669\n",
      "           1       0.93      0.96      0.94     15331\n",
      "\n",
      "    accuracy                           0.91     20000\n",
      "   macro avg       0.89      0.86      0.88     20000\n",
      "weighted avg       0.91      0.91      0.91     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create and print classification report of model\n",
    "# this allows us to evaluate the performance of both classes\n",
    "# this is important due to class imbalance\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9608737032193948\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for the test data using the XGBoost classifier\n",
    "y_pred_proba = best_xgb_clf.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# calculate the AUC-ROC metric\n",
    "# this is valuable due to class imbalance\n",
    "auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC-ROC: {auc_roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboostv1.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump model to file that can be reloaded\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(best_xgb_clf, \"xgboostv1.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
